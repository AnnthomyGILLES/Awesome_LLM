# Awesome LLM - Personal review

## Vector databases

 - [Vector databases (Part 1): What makes each one different?](https://thedataquarry.com/posts/vector-db-1/)
 - [Vector databases (Part 2): Understanding their internals](https://thedataquarry.com/posts/vector-db-1/)
	 - The article delves into the rising significance of vector databases in the era of Large Language Models (LLMs) like ChatGPT. While LLMs can generate coherent text, they sometimes produce inaccurate results. Vector databases, which store data as vectors, can be combined with LLMs to provide more accurate and up-to-date information. The piece also touches on the technical aspects of vector databases, including embeddings, similarity computation, indexing, and the potential of hybrid search systems that combine traditional keyword and vector searches.

## Architecture & Development
 - [Building Scalable Large Language Model (LLM) Apps](https://ai.plainenglish.io/building-scalable-large-language-model-llm-apps-509894bc7f6a)
	 - The article delves into building scalable large language model (LLM) applications. It emphasizes FastAPI's benefits and Langchain's scalability challenges. It recommends vendor-provided VectorStores or using Redis and Qdrant for scalability, underscores the significance of semantic caching with GPTCache, and suggests Microsoft Guidance for controlled LLM outputs. For document processing, unstructured.io is highlighted as a top recommendation.
	
## LLMs Limitations & Challenges

 - [Open challenges in LLM research](https://huyenchip.com/2023/08/16/llm-research-open-challenges.html)
	 - The article delves into the current challenges and research directions concerning Large Language Models (LLMs). It highlights 10 key areas: 1) Reducing AI's tendency to fabricate information (hallucinations), 2) Optimizing context understanding and response generation, 3) Incorporating multiple data types like text and images (multimodality), 4) Enhancing LLMs' speed and cost-efficiency, 5) Designing new model architectures beyond the Transformer, 6) Exploring alternatives to traditional GPU-based deep learning, 7) Making AI agents that can perform tasks more usable, 8) Refining models' learning based on human preferences, 9) Improving the efficiency of chat interfaces, and 10) Developing LLMs for languages other than English. 

