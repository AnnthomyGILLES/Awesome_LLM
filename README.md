
# Awesome LLM - Personal review
## Background
- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
## Vector databases

 - [Vector databases (Part 1): What makes each one different?](https://thedataquarry.com/posts/vector-db-1/)
 - [Vector databases (Part 2): Understanding their internals](https://thedataquarry.com/posts/vector-db-1/): The article delves into the rising significance of vector databases in the era of Large Language Models (LLMs) like ChatGPT. While LLMs can generate coherent text, they sometimes produce inaccurate results. Vector databases, which store data as vectors, can be combined with LLMs to provide more accurate and up-to-date information. The piece also touches on the technical aspects of vector databases, including embeddings, similarity computation, indexing, and the potential of hybrid search systems that combine traditional keyword and vector searches.

## Theoretical Understanding

### Transformer and Attention
- [√Ä la d√©couverte du Transformer | Le Data Scientist](https://ledatascientist.com/a-la-decouverte-du-transformer/)
- [Transformer‚Äôs Encoder-Decoder](https://kikaben.com/transformers-encoder-decoder/)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html): The Transformer model from "Attention is All You Need" has revolutionized NLP tasks. This article provides a streamlined, annotated implementation of the paper, consisting of 400 lines of code.

	
## LLMs Limitations & Challenges

 - [Open challenges in LLM research](https://huyenchip.com/2023/08/16/llm-research-open-challenges.html): The article delves into the current challenges and research directions concerning Large Language Models (LLMs). It highlights 10 key areas: 1) Reducing AI's tendency to fabricate information (hallucinations), 2) Optimizing context understanding and response generation, 3) Incorporating multiple data types like text and images (multimodality), 4) Enhancing LLMs' speed and cost-efficiency, 5) Designing new model architectures beyond the Transformer, 6) Exploring alternatives to traditional GPU-based deep learning, 7) Making AI agents that can perform tasks more usable, 8) Refining models' learning based on human preferences, 9) Improving the efficiency of chat interfaces, and 10) Developing LLMs for languages other than English.

## Build
 - [Building Scalable Large Language Model (LLM) Apps](https://ai.plainenglish.io/building-scalable-large-language-model-llm-apps-509894bc7f6a)
	 - The article delves into building scalable large language model (LLM) applications. It emphasizes FastAPI's benefits and Langchain's scalability challenges. It recommends vendor-provided VectorStores or using Redis and Qdrant for scalability, underscores the significance of semantic caching with GPTCache, and suggests Microsoft Guidance for controlled LLM outputs. For document processing, unstructured.io is highlighted as a top recommendation.
 - [ü¶úÔ∏è LangChain + Streamlitüî•+ Llama ü¶ô: Bringing Conversational AI to Your Local Machine ü§Ø](https://ai.plainenglish.io/%EF%B8%8F-langchain-streamlit-llama-bringing-conversational-ai-to-your-local-machine-a1736252b172)
	 - Integrating Open Source LLMs and LangChain for Free Generative Question Answering (No API Key required)
 - [All You Need to Know to Build Your First LLM App](https://towardsdatascience.com/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac)
	 - Integrating Open Source LLMs and LangChain for Free Generative Question Answering (No API Key required)


